# -*- coding: utf-8 -*-
"""CUSTOMER CHURN PREDICTION (LR,KNN,DT,RF,SVR,GB).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NngViYw9ILEdoZnXruPlAOK2xcghv3p9

# **CUSTOMER CHURN PREDICTION (LR,KNN,DT,RF,SVR,GB)**

<h1 style="font-family: 'poppins'; font-weight: bold; color: Green;">ðŸ‘¨ðŸ’»Author: Irfan Ullah Khan</h1>

[![GitHub](https://img.shields.io/badge/GitHub-Profile-blue?style=for-the-badge&logo=github)](https://github.com/programmarself)
[![Kaggle](https://img.shields.io/badge/Kaggle-Profile-blue?style=for-the-badge&logo=kaggle)](https://www.kaggle.com/programmarself)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/irfan-ullah-khan-4a2871208/)  
[![YouTube](https://img.shields.io/badge/YouTube-Profile-red?style=for-the-badge&logo=youtube)](https://www.youtube.com/@irfanullahkhan7748)
[![Email](https://img.shields.io/badge/Email-Contact%20Me-red?style=for-the-badge&logo=email)](mailto:programmarself@gmail.com)
[![Website](https://img.shields.io/badge/Websit-Contact%20Me-red?style=for-the-badge&logo=Website)](https://datasciencetoyou.odoo.com/)
"""

# @title
!pip install catboost

# loading necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import pyplot
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from catboost import CatBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score,recall_score
from xgboost import XGBClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score, GridSearchCV

df = pd.read_csv("BankCustomerData.csv")

"""# Exploratory Data Analysis"""

df.head()

df.shape

df.info()

df.describe()

categorical_variables = [col for col in df.columns if col in "O"
                        or df[col].nunique() <=11
                        and col not in "churn"]

categorical_variables

numeric_variables = [col for col in df.columns if df[col].dtype != "object"
                        and df[col].nunique() >11
                        and col not in "customer_id"]
numeric_variables

df["churn"].value_counts()

exit = df.loc[df["churn"]==1]
not_exit = df.loc[df["churn"]==0]

not_exit.shape
#exit.shape

def get_sorted_value_counts(df, column_name):
    return df[column_name].value_counts().sort_values()

print('Tenure frequency of the churned and not churned groups')
print(get_sorted_value_counts(not_exit, "tenure"))
print(get_sorted_value_counts(exit, "tenure"))

print('Number of products frequency of the churned and not churned groups')
print(get_sorted_value_counts(not_exit, "products_number"))
print(get_sorted_value_counts(exit, "products_number"))

print('credit card frequency of the churned and not churned groups')
print(get_sorted_value_counts(not_exit, "credit_card"))
print(get_sorted_value_counts(exit, "credit_card"))

print('If active based frequency of the churned and not churned groups')
print(get_sorted_value_counts(not_exit, "active_member"))
print(get_sorted_value_counts(exit, "active_member"))

print('country frequency of the churned and not churned groups')
print(get_sorted_value_counts(not_exit, "country"))
print(get_sorted_value_counts(exit, "country"))

print('gender frequency of the churned and not churned groups')
print(get_sorted_value_counts(not_exit, "gender"))
print(get_sorted_value_counts(exit, "gender"))

# distribution of the Credit Score for not_exit
pyplot.figure(figsize=(8,6))
pyplot.xlabel('credit_score')
pyplot.hist(not_exit["credit_score"],bins=15, alpha=0.7, label='Not Exit')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Credit Score for exit
pyplot.figure(figsize=(8,6))
pyplot.xlabel('credit_score')
pyplot.hist(exit["credit_score"],bins=15, alpha=0.8, label='Exit')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Age for not_exit
pyplot.figure(figsize=(8,6))
pyplot.xlabel('age')
pyplot.hist(not_exit["age"],bins=15, alpha=0.7, label='Not Exit')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Age for exit
pyplot.figure(figsize=(8,6))
pyplot.xlabel('age')
pyplot.hist(exit["age"],bins=15, alpha=0.7, label='Exit')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Balance for churn
pyplot.figure(figsize=(8,6))
pyplot.xlabel('balance')
pyplot.hist(exit["balance"],bins=15, alpha=0.7, label='Exit')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the Balance for not_exit
pyplot.figure(figsize=(8,6))
pyplot.xlabel('balance')
pyplot.hist(not_exit["balance"],bins=15, alpha=0.7, label='Not Exit')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the estimated_salary for exit
pyplot.figure(figsize=(8,6))
pyplot.xlabel('estimated_salary')
pyplot.hist(not_exit["estimated_salary"],bins=15, alpha=0.7, label='Not exit')
pyplot.legend(loc='upper right')
pyplot.show()

# distribution of the estimated_salary for exit
pyplot.figure(figsize=(8,6))
pyplot.xlabel('estimated_salary')
pyplot.hist(exit["estimated_salary"],bins=15, alpha=0.7, label='exit')
pyplot.legend(loc='upper right')
pyplot.show()

df.head()

df.dtypes

df["churn"] = df["churn"].astype("category")
df["credit_score"] = df["credit_score"].astype("category")

sns.catplot(x="churn", y="credit_score", data=df)

sns.catplot(x="churn", y="age", data = df)

sns.catplot(x="churn", y="balance", data = df)

sns.catplot(x="churn", y="estimated_salary", data = df)

"""# Data Preprocessing"""

df.isnull().sum()

"""There are no missing values in this dataset

There are no outliers in this dataset
"""

# Variables to apply one hot encoding
list = ["gender", "country"]
df = pd.get_dummies(df, columns =list, drop_first = True)

df.head()

"""# Scalling"""

df = df.drop(["customer_id"], axis = 1)

df

scaler = RobustScaler()
scaled_data = scaler.fit_transform(df)

"""# Modeling"""

X = df.drop("churn",axis=1)
y = df["churn"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

models = [('LR', LogisticRegression(random_state=42)),
          ('KNN', KNeighborsClassifier()),
          ('DT', DecisionTreeClassifier(random_state=42)),
          ('RF', RandomForestClassifier(random_state=42)),
          ('SVR', SVC(gamma='auto',random_state=42)),
          ('GB', GradientBoostingClassifier(random_state = 42)),
          ("LightGBM", LGBMClassifier(random_state=42))]
results = []
names = []
for name, model in models:
    kfold = KFold(n_splits=10)
    cv_results = cross_val_score(model, X, y, cv=kfold)
    results.append(cv_results)
    names.append(name)
    output = "%s: %f " % (name, cv_results.mean())
    print(output)

def classifier_results(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred,zero_division=0)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    roc_auc = roc_auc_score(y_test, pred_proba)
    print("Accuracy: {:.4f} Precision: {:.4f} Recall: {:.4f} F1: {:.4f} ROC-AUC: {:.4f}".format(accuracy,precision,recall,f1,roc_auc))
    plt.figure(figsize=(8, 6))
    ax = sns.heatmap(confusion, cmap = 'YlGnBu',annot = True, fmt='d')
    ax.set_title('Confusion Matrix')

    return confusion

def generate_auc_roc_curve(y_test,  y_pred_proba):
    fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr,tpr,label="AUC ROC Curve with Area Under the curve ="+str(auc))
    plt.legend(loc=4)
    plt.show()
    pass

model = GradientBoostingClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]
classifier_results(y_test, pred=y_pred, pred_proba=y_pred_proba)

generate_auc_roc_curve(y_test,  y_pred_proba)

"""#Â Hyperparameter Tuning"""

def grid_search_cv(estimator, param_grid, cv=5, scoring='roc_auc'):
    grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cv, scoring=scoring)
    grid_search.fit(X_train, y_train)
    return grid_search.best_params_

"""Lets tune the LGBMClassifier,GradientBoostingClassifier and RandomForestClassifier"""

lgbm_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.1, 0.05, 0.01],
    'max_depth': [3, 5, 7]
}

gb_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.1, 0.05, 0.01],
    'max_depth': [3, 5, 7]
}

rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7]
}

lgbm = LGBMClassifier(random_state=42)
lgbm_best_params = grid_search_cv(lgbm, lgbm_param_grid)
print("Best parameters for LGBMClassifier:", lgbm_best_params)

gb = GradientBoostingClassifier(random_state=42)
gb_best_params = grid_search_cv(gb, gb_param_grid)
print("Best parameters for GradientBoostingClassifier:", gb_best_params)

rf = RandomForestClassifier(random_state=42)
rf_best_params = grid_search_cv(rf, rf_param_grid)
print("Best parameters for RandomForestClassifier:", rf_best_params)

model1 = LGBMClassifier(learning_rate= 0.05, max_depth= 5, n_estimators= 100,random_state=42)
model1.fit(X_train, y_train)
y_pred = model1.predict(X_test)
y_pred_proba = model1.predict_proba(X_test)[:, 1]
classifier_results(y_test, pred=y_pred, pred_proba=y_pred_proba)

model2 = GradientBoostingClassifier(learning_rate= 0.05, max_depth= 3, n_estimators= 200,random_state=42)
model2.fit(X_train, y_train)
y_pred = model2.predict(X_test)
y_pred_proba = model2.predict_proba(X_test)[:, 1]
classifier_results(y_test, pred=y_pred, pred_proba=y_pred_proba)

model3 = RandomForestClassifier(max_depth= 7, n_estimators= 200,random_state=42)
model3.fit(X_train, y_train)
y_pred = model3.predict(X_test)
y_pred_proba = model3.predict_proba(X_test)[:, 1]
classifier_results(y_test, pred=y_pred, pred_proba=y_pred_proba)

"""# Conclusion

The Best model is LGBMClassifier model with a learning rate of 0.05, max depth of 5, and 100 estimators achieved an accuracy of 0.8670. It showed a precision of 0.7571, recall of 0.4758, and an F1 score of 0.5844. The model's ROC-AUC score was 0.8741, indicating good performance in distinguishing between positive and negative cases. Overall, the model demonstrated promising results, although further improvements can be made. Trialing wiht ensembles may result in better results
"""